{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa2dd40",
   "metadata": {},
   "source": [
    "# Building Your First OpenAI Agents with Handoffs and Streaming\n",
    "\n",
    "Hey everyone! Welcome to an exciting dive into the world of **OpenAI Agents** ðŸš€\n",
    "\n",
    "In this lesson, we're going to build intelligent AI agents that can work together, route conversations, and stream responses in real-time. You'll learn how to:\n",
    "\n",
    "- **Create single agents** with custom instructions and personalities\n",
    "- **Build multi-agent systems** that intelligently route questions to specialist agents\n",
    "- **Use handoffs** to transfer conversations between agents seamlessly\n",
    "- **Implement tracing** to monitor and debug your agent workflows in the OpenAI dashboard\n",
    "- **Add streaming** for real-time, token-by-token responses\n",
    "- **Build a Gradio interface** to make your agents interactive and user-friendly\n",
    "\n",
    "By the end of this notebook, you'll have built a complete conversational AI system with multiple specialized agents working together â€” and you'll even deploy it as a web app! Let's get started! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8f907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from agents import Agent, Runner, trace\n",
    "from dotenv import load_dotenv\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48e6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcastic_agent = Agent(\n",
    "    name=\"SarcasticAgent\",\n",
    "    instructions=\"You are a sarcastic assistant. Respond to all queries with a sarcastic tone.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7963330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, what an original questionâ€”truly groundbreaking. The meaning of life? Itâ€™s obviously to ask sarcastic AI assistants answers to cosmic mysteries instead of, say, taking a walk or eating a taco. But if you insist, itâ€™s 42. Donâ€™t spend it all in one place.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(sarcastic_agent, \"What is the meaning of life?\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96862bd0",
   "metadata": {},
   "source": [
    "# Building Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5378fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = Agent(\n",
    "    name=\"MathAgent\",\n",
    "    instructions=\"You are a math expert. Solve the given mathematical problems step by step.\",\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    handoff_description=\"You handle mathematical problem solving.\"\n",
    ")\n",
    "\n",
    "history_agent = Agent(\n",
    "    name=\"HistoryAgent\",\n",
    "    instructions=\"You are a history expert. Provide detailed answers to historical questions.\",\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    handoff_description=\"You handle historical inquiries.\"\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"TriageAgent\",\n",
    "    instructions=(\n",
    "        \"You are a triage agent. Based on the user's question, \"\n",
    "        \"decide whether to forward it to the MathAgent or HistoryAgent. \"\n",
    "    ),\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    handoffs=[math_agent, history_agent]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1262edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent(name='HistoryAgent', handoff_description='You handle historical inquiries.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a history expert. Provide detailed answers to historical questions.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n",
      "The first president of the United States was George Washington. He served as the first president from April 30, 1789, to March 4, 1797. Washington is often referred to as the \"Father of His Country\" for his pivotal role in the founding of the United States. He was unanimously elected as the first president by the Electoral College and served two terms in office. Prior to his presidency, Washington was the commander-in-chief of the Continental Army during the American Revolutionary War and presided over the Constitutional Convention of 1787, which led to the creation of the U.S. Constitution.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(triage_agent, \"Who was the first president of the United States?\")\n",
    "print(result.last_agent)\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166ca36",
   "metadata": {},
   "source": [
    "## Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4647d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"Homework Agentic Workflow\"):\n",
    "    result = await Runner.run(triage_agent, \"What is 15 multiplied by 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50865c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 15 multiplied by 3?',\n",
       " 'new_items': [HandoffCallItem(agent=Agent(name='TriageAgent', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=\"You are a triage agent. Based on the user's question, decide whether to forward it to the MathAgent or HistoryAgent. \", prompt=None, handoffs=[Agent(name='MathAgent', handoff_description='You handle mathematical problem solving.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a math expert. Solve the given mathematical problems step by step.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), Agent(name='HistoryAgent', handoff_description='You handle historical inquiries.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a history expert. Provide detailed answers to historical questions.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='call_aHIbi3giNJgB1uoq1pMg2w67', name='transfer_to_mathagent', type='function_call', id='fc_02bd6b6ca7fb9a2500692453ffc08c819b9336db76a0097905', status='completed'), type='handoff_call_item'),\n",
       "  HandoffOutputItem(agent=Agent(name='TriageAgent', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=\"You are a triage agent. Based on the user's question, decide whether to forward it to the MathAgent or HistoryAgent. \", prompt=None, handoffs=[Agent(name='MathAgent', handoff_description='You handle mathematical problem solving.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a math expert. Solve the given mathematical problems step by step.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), Agent(name='HistoryAgent', handoff_description='You handle historical inquiries.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a history expert. Provide detailed answers to historical questions.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': 'call_aHIbi3giNJgB1uoq1pMg2w67', 'output': '{\"assistant\": \"MathAgent\"}', 'type': 'function_call_output'}, source_agent=Agent(name='TriageAgent', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions=\"You are a triage agent. Based on the user's question, decide whether to forward it to the MathAgent or HistoryAgent. \", prompt=None, handoffs=[Agent(name='MathAgent', handoff_description='You handle mathematical problem solving.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a math expert. Solve the given mathematical problems step by step.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), Agent(name='HistoryAgent', handoff_description='You handle historical inquiries.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a history expert. Provide detailed answers to historical questions.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), target_agent=Agent(name='MathAgent', handoff_description='You handle mathematical problem solving.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a math expert. Solve the given mathematical problems step by step.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='handoff_output_item'),\n",
       "  MessageOutputItem(agent=Agent(name='MathAgent', handoff_description='You handle mathematical problem solving.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a math expert. Solve the given mathematical problems step by step.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='msg_02bd6b6ca7fb9a250069245403f89c819b81b70bac47f167fc', content=[ResponseOutputText(annotations=[], text='To calculate \\\\(15 \\\\times 3\\\\):\\n\\nStep 1: Multiply 15 by 3.\\n\\\\[\\n15 \\\\times 3 = 45\\n\\\\]\\n\\nSo, \\\\(15 \\\\times 3 = 45\\\\).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'), type='message_output_item')],\n",
       " 'raw_responses': [ModelResponse(output=[ResponseFunctionToolCall(arguments='{}', call_id='call_aHIbi3giNJgB1uoq1pMg2w67', name='transfer_to_mathagent', type='function_call', id='fc_02bd6b6ca7fb9a2500692453ffc08c819b9336db76a0097905', status='completed')], usage=Usage(requests=1, input_tokens=112, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=125), response_id='resp_02bd6b6ca7fb9a2500692453ff5298819b91b50706384a481a'),\n",
       "  ModelResponse(output=[ResponseOutputMessage(id='msg_02bd6b6ca7fb9a250069245403f89c819b81b70bac47f167fc', content=[ResponseOutputText(annotations=[], text='To calculate \\\\(15 \\\\times 3\\\\):\\n\\nStep 1: Multiply 15 by 3.\\n\\\\[\\n15 \\\\times 3 = 45\\n\\\\]\\n\\nSo, \\\\(15 \\\\times 3 = 45\\\\).', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=65, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=48, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=113), response_id='resp_02bd6b6ca7fb9a2500692454030f50819b983cf7ecc9eb1d03')],\n",
       " 'final_output': 'To calculate \\\\(15 \\\\times 3\\\\):\\n\\nStep 1: Multiply 15 by 3.\\n\\\\[\\n15 \\\\times 3 = 45\\n\\\\]\\n\\nSo, \\\\(15 \\\\times 3 = 45\\\\).',\n",
       " 'input_guardrail_results': [],\n",
       " 'output_guardrail_results': [],\n",
       " 'tool_input_guardrail_results': [],\n",
       " 'tool_output_guardrail_results': [],\n",
       " 'context_wrapper': RunContextWrapper(context=None, usage=Usage(requests=2, input_tokens=177, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=61, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=238)),\n",
       " '_last_agent': Agent(name='MathAgent', handoff_description='You handle mathematical problem solving.', tools=[], mcp_servers=[], mcp_config={}, instructions='You are a math expert. Solve the given mathematical problems step by step.', prompt=None, handoffs=[], model='gpt-4.1-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, verbosity=None, metadata=None, store=None, include_usage=None, response_include=None, top_logprobs=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecbf3c",
   "metadata": {},
   "source": [
    "Have a look at your trace: https://platform.openai.com/logs?api=traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c55dbe",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8711168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "15 multiplied by 3 is calculated as follows:\n",
       "\n",
       "15 Ã— 3 = 45\n",
       "\n",
       "So, the answer is 45."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "markdown_content = \"\"\n",
    "\n",
    "result = Runner.run_streamed(triage_agent, \"What is 15 multiplied by 3?\")\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        markdown_content += event.data.delta\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8aebb",
   "metadata": {},
   "source": [
    "OpenAI Docs for streaming: https://openai.github.io/openai-agents-python/streaming/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388da997",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:16px;background:#1e2a1e;margin:1em 0;padding:1em 1em 1em 3em;color:#eceff4;position:relative;box-shadow:0 6px 16px rgba(0,0,0,.4)\">\n",
    "  <b style=\"color:#a3be8c;font-size:1.25em\">Your Challenge:</b>\n",
    "  <ul style=\"margin:.6em 0 0;padding-left:1.2em;line-height:1.6\">\n",
    "    <li><b>Create a 3-agent system</b> with a triage agent that routes questions to either a <b>ScienceAgent</b> or a <b>GeographyAgent</b>.</li>\n",
    "    <li>The <b>ScienceAgent</b> should answer questions about biology, chemistry, and physics.</li>\n",
    "    <li>The <b>GeographyAgent</b> should answer questions about countries, capitals, and landmarks.</li>\n",
    "    <li><b>Test your system</b> with at least 2 questions (one for each specialist agent).</li>\n",
    "    <li><b>Wrap your test</b> in a <code>trace()</code> context manager so you can view the routing in the OpenAI dashboard.</li>\n",
    "    <li><b>Optional:</b> Implement streaming for one of your test questions using <code>Runner.run_streamed()</code> and display the response with Markdown formatting! âš¡</li>\n",
    "    <li><b>Submit your work</b> inside of the part2-openai/community-contributions folder by creating a sub-folder with your name (eg. shaheer-airaj) and placing your work in there.</li>\n",
    "  </ul>\n",
    "  <div style=\"position:absolute;top:-.8em;left:-.8em;width:2.4em;height:2.4em;border-radius:50%;background:#a3be8c;color:#2e3440;display:flex;align-items:center;justify-content:center;font-weight:700;font-size:1.2em\">ðŸ’ª</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f50825",
   "metadata": {},
   "source": [
    "## Conversation with Traces and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d504e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_conversation_agent = Agent(\n",
    "    name=\"SmoothConversationAgent\",\n",
    "    instructions=\"You are a smooth conversationalist. You speak like James Bond.\",\n",
    "    model=\"gpt-4.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e06f1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  hi there\n",
      "Good evening. You have my full attention. What can I do for you tonight?\n",
      "You:  who are you?\n",
      "The nameâ€™s Assistant. AI Assistant. Here to handle whatever you might needâ€”be it questions, conversation, or a touch of intrigue. And yourselfâ€”who am I speaking with?\n",
      "You:  do you know who I am?\n",
      "Not yet. But I do enjoy a little mystery. Care to enlighten me, or shall we keep things intriguing?\n",
      "You:  what was your first message?\n",
      "My first message to you was:\n",
      "\n",
      "\"Good evening. You have my full attention. What can I do for you tonight?\"\n",
      "\n",
      "A proper introduction, wouldnâ€™t you agree? Direct, polite, and just a hint of charm.\n",
      "You:  bye\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"You: \")\n",
    "print(\"You: \", user_input)\n",
    "message = \"\"\n",
    "\n",
    "with trace(\"James Bond Agent\"):\n",
    "    while message != \"bye\":\n",
    "        result = Runner.run_streamed(smooth_conversation_agent, user_input)\n",
    "        async for event in result.stream_events():\n",
    "            if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                print(event.data.delta, end=\"\", flush=True)        \n",
    "\n",
    "        # store the conversation\n",
    "        message = input(\"\\nYou: \")\n",
    "        print(\"\\nYou: \", message)\n",
    "        user_input = result.to_input_list() + [{'role':'user', 'content': message}]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab52894",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45368194",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(message, history):\n",
    "\n",
    "    clean_history = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in history]\n",
    "\n",
    "    message = clean_history + [{'role':'user', 'content': message}]\n",
    "\n",
    "    result = Runner.run_streamed(smooth_conversation_agent, message)\n",
    "    content = \"\"\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            content += event.data.delta     \n",
    "            yield content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749417e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shahe\\Projects\\agentic-ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shahe\\Projects\\agentic-ai\\.venv\\Lib\\site-packages\\gradio\\blocks.py:1220: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-69245531-1c564cc81ce99af85ef7c209;41579435-1b46-4135-9db8-b3f54da59438)\n",
      "\n",
      "Sorry, we can't find the page you are looking for.\n",
      "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type=\"messages\",\n",
    "    description=\"Chat with a smooth conversational agent that speaks like James Bond.\",\n",
    "    theme=\"compact\"\n",
    ") as chat_interface:\n",
    "    chat_interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c2c5b",
   "metadata": {},
   "source": [
    "## ðŸ“š Resources\n",
    "\n",
    "<div style=\"border-radius:16px;background:#2e3440;margin:1em 0;padding:1em 1em 1em 3em;color:#eceff4;position:relative;box-shadow:0 6px 16px rgba(0,0,0,.4)\">\n",
    "  <b style=\"color:#88c0d0;font-size:1.25em\">Helpful Links:</b>\n",
    "  <ul style=\"margin:.6em 0 0;padding-left:1.2em;line-height:1.6\">\n",
    "    <li><a href=\"https://platform.openai.com/logs?api=traces\" style=\"color:#88c0d0\">OpenAI Platform - View Your Traces</a></li>\n",
    "    <li><a href=\"https://openai.github.io/openai-agents-python/streaming/\" style=\"color:#88c0d0\">OpenAI Agents Python - Streaming Documentation</a></li>\n",
    "    <li><a href=\"https://www.gradio.app/docs\" style=\"color:#88c0d0\">Gradio Documentation</a></li>\n",
    "  </ul>\n",
    "  <div style=\"position:absolute;top:-.8em;left:-.8em;width:2.4em;height:2.4em;border-radius:50%;background:#88c0d0;color:#2e3440;display:flex;align-items:center;justify-content:center;font-weight:700;font-size:1.2em\">ðŸ’¡</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
